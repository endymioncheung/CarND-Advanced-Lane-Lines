{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: **Advanced Lane Finding** \n",
    "[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n",
    "\n",
    "<img src=\"examples/example_output.jpg\" width=\"720\" alt=\"Combined Image\" />\n",
    "\n",
    "\n",
    "## Overview\n",
    "---\n",
    "\n",
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The main goal of this project is to use front-facing camera mounted in the car to detect lane lines in real-time, highlighting the lane the car is in, as well as providing lane curvature radius and the position of the vehicle in the lane.\n",
    "\n",
    "The steps of the pipeline used this project are as follow:\n",
    "\n",
    "1. Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "2. Apply a distortion correction to raw images.\n",
    "3. Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "4. Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "5. Detect lane pixels and fit to find the lane boundary.\n",
    "6. Determine the curvature of the lane and vehicle position with respect to center.\n",
    "7. Warp the detected lane boundaries back onto the original image.\n",
    "8. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "9. Putting all the above image processing steps together into a single a process_image function\n",
    "10. Apply the process image function to video clip or stream\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/undistort_output.png \"Undistorted\"\n",
    "[image2]: ./test_images/test1.jpg \"Road Transformed\"\n",
    "[image3]: ./examples/binary_combo_example.jpg \"Binary Example\"\n",
    "[image4]: ./examples/warped_straight_lines.jpg \"Warp Example\"\n",
    "[image5]: ./examples/color_fit_lines.jpg \"Fit Visual\"\n",
    "[image6]: ./examples/example_output.jpg \"Output\"\n",
    "[video1]: ./project_video.mp4 \"Video\"\n",
    "\n",
    "---\n",
    "\n",
    "# Setup\n",
    "\n",
    "This project is programmed using Python 3 in Jupyter Notebook. Some common open source frameworks were used in this project:\n",
    "\n",
    "* `openCV2`: Computer vision processing library\n",
    "* `numpy`: Array and matrices manipulation\n",
    "* `matplotlib`: Plot graphs or show images\n",
    "* `glob`: Create list of files\n",
    "* `pickle`: Save/load raw format workspace data\n",
    "* `moviepy`: Access to read, edit and write to video\n",
    "\n",
    "\n",
    "### Camera Calibration - WHY and HOW?\n",
    "\n",
    "Before beginning any camera calibration, it is important to start off with a camera source as unbiased as possible. Due to the inevitable nature of any cameras, there will always be some distortions. The amount of distortion varies. \n",
    "\n",
    "The purpose of this function is to compute the camera calibration, which is a set of coefficients based on a sample pool of camera calibration images. These camera calibration images are in the format of 9x6 chessboard. Chessboard images were selected for calibration because it has a regular high contrast pattern, which makes it very easy for us to find corners upfront.\n",
    "\n",
    "In order to calculate the camera calibration coefficients, we first need to identify a set of 2D image points and 3D object points for the corners. For the 2D image points, since we know it is a 9x6 chessboard, assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image, we can define object points accordingly like one implemented below. For the 3D object points, we can use the `cv2.findChessboardCorners()`.\n",
    "\n",
    "Once we have the image and object points, we can then use `cv2.calibrateCamera()` to finally compute the camera calibration coefficient. We will use these coefficient to undistort the disorted images straight from camera.\n",
    "\n",
    "I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: \n",
    "\n",
    "![alt text](output_images/disort_undisort_image.png)\n",
    "\n",
    "```python\n",
    "def map_obj_img_points(img_name, objpoints, imgpoints, visible=True, h_corners=9, v_corners=6):\n",
    "    # Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((9*6,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "    img = cv2.imread(img_name)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (h_corners,v_corners),None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "    else:\n",
    "        print(\"Unable to detect corners for image : \" + img_name )\n",
    "    return objpoints, imgpoints\n",
    "\n",
    "def compute_camera_calibration(calibration_images):\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane\n",
    "    \n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob(calibration_images)\n",
    "    \n",
    "    # Step through the list and search for chessboard corners\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        objpoints, imgpoints = map_obj_img_points(fname,objpoints,imgpoints,visible=False)\n",
    "\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    # Do camera calibration given object points and image points\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "    return ret, mtx, dist, rvecs, tvecs\n",
    "\n",
    "def undistort_camera_img(img,mtx,dist):   \n",
    "    undist_img = cv2.undistort(img, mtx, dist, None, mtx)    \n",
    "```\n",
    "\n",
    "### Pipeline (single images)\n",
    "\n",
    "#### 1. Distortion correcting image\n",
    "\n",
    "After obtaining the camera calibration using `cv2.calibrateCamera()`, now I can apply distortion correction to other test images using `cv2.undistort()`, like one shown below.\n",
    "\n",
    "![alt text](output_images/dist_undist_test5.png)\n",
    "\n",
    "#### 2. Thresholded binary image\n",
    "\n",
    "In order to extract useful features from the image, it is common to use a combination thresholded binary image techniques. I used a combination of **color and gradient thresholds** filters to generate a single binary image. This way I can get as much information about the lane lines from the image as possible. \n",
    "\n",
    "For the color threshold binary image extraction, I chose to work in the **HLS color space** rather than the traditional RGB color space because the HLS color space can pick up **not only the white but also the yellow lines robustly** under various light changing conditions. It's worth noting, however, that the R (red) channel still does rather well on the white lines, perhaps even better than the S (saturation) channel. As with gradients, it's worth considering how you might combine various color thresholds to make the most robust identification of the lines.\n",
    "\n",
    "This is an example of binary image using the saturation color filter on a test image using `saturation_thresh(img,s_thresh)`\n",
    "\n",
    "```python\n",
    "def saturation_thresh(img,s_thresh):\n",
    "    # Convert to HLS color space and separate the S channel\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    h_channel = hls[:,:,0]\n",
    "    l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    \n",
    "    # Threshold saturation channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "\n",
    "    return s_binary\n",
    "```\n",
    "\n",
    "![alt text](output_images/s_binary_test5.png)\n",
    "\n",
    "Similarly, I used the directional gradient filter to extract the close to vertical feature in the test image using `abs_sobel_thresh(img, orient, sobel_kernel, thresh)` to obtain the gradient-directional binary image below.\n",
    "\n",
    "```python\n",
    "def abs_sobel_thresh(img, orient, sobel_kernel, thresh):\n",
    "    # Calculate directional gradient\n",
    "    # Apply threshold\n",
    "\n",
    "    # 1) Convert to grayscale\n",
    "    # 2) Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    # 3) Take the absolute value of the derivative or gradient\n",
    "    # 4) Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    # 5) Create a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    if (orient == 'x'):\n",
    "        # Take the derivative in x\n",
    "        sobel = cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=sobel_kernel)\n",
    "    elif (orient == 'y'):\n",
    "        # Take the derivative in y\n",
    "        sobel = cv2.Sobel(gray,cv2.CV_64F,0,1,ksize=sobel_kernel)\n",
    "    \n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    \n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return grad_binary\n",
    "```\n",
    "\n",
    "![alt text](output_images/sx_binary_test5.png)\n",
    "\n",
    "To put the combined image binary extraction together, I can simply sum them up using `combine_binary_thresh(binary_1,binary_2)`. Below the binary image on the left highlights the two filters stacking one on top of another and the binary image on the right show the final combined binary image, which I use for the next pipeline for perspective transform it to an bird-eye view.\n",
    "\n",
    "```python\n",
    "def combine_binary_thresh(binary_1,binary_2):\n",
    "    # Combine binary thresholds (saturation and directional gradient)\n",
    "    combined_binary = np.zeros_like(binary_1)\n",
    "    combined_binary[(binary_1 == 1) | (binary_2 == 1)] = 1\n",
    "    \n",
    "    return combined_binary\n",
    "```\n",
    "\n",
    "![alt text](output_images/stacked_combined_test5.png)\n",
    "\n",
    "#### 3. Perspective Transform\n",
    "\n",
    "Apply a perspective transform to rectify binary image (\"birds-eye view\"). Using the two straight lines images to verify that the perspective transform is correct. That is the lane lines in the images aligned with the overlayed vertical lines drawn as shown below.\n",
    "\n",
    "```python\n",
    "src = np.float32(\n",
    "        [[245,  680],  # Bottom left\n",
    "         [580,  460],  # Top left\n",
    "         [710,  460],  # Top right\n",
    "         [1060, 680]]) # Bottom right\n",
    "\n",
    "dst = np.float32(\n",
    "        [[245,  680],  # Bottom left\n",
    "         [245,    0],  # Top left\n",
    "         [1060,   0],  # Top right\n",
    "         [1060, 680]]) # Bottom right\n",
    "M = cv2.getPerspectiveTransform(src,dst)\n",
    "\n",
    "def warp(img,M):\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "    # Create warped image using linear interpolation\n",
    "    warped_img = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped_img\n",
    "```\n",
    "\n",
    "This resulted in the following source and destination points:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 245, 680      | 245, 680      | \n",
    "| 580, 460      | 245, 0        |\n",
    "| 710, 460      | 1060, 0       |\n",
    "| 1060, 680     | 1060, 680     |\n",
    "\n",
    "I verified that my perspective transform was working as expected by drawing the `src` and `dst` points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image.\n",
    "\n",
    "![alt text](output_images/perspective_straight_lines1_binary.png)\n",
    "![alt text](output_images/perspective_straight_lines2_binary.png)\n",
    "\n",
    "#### 4. Detect Lane Lines\n",
    "\n",
    "The detection of the lane lines are broken into two steps:\n",
    "\n",
    "    1. Use histogram to find left and right lanes\n",
    "    2. Use 2nd order polynomial to fit the left and right lanes\n",
    "\n",
    "#### 4.1 Find left and right lanes\n",
    "\n",
    "To find the lane lines, I first took the bottom half of the image and located the **peaks of the left and right halves of the histogram** along the horziontal axis. These will be the starting point for the left and right lines.\n",
    "\n",
    "The x index of the histogram peaks is used to specify the base location of our sliding windows where I looked for the lane lines. Then I created two lists to add only the non-zero pixels within the sliding windows for the left and right lanes.\n",
    "\n",
    "If the number of the pixels found in the sliding window is greater than the minimum pixels specified in the window, recenter next window on their mean position. Continue the non-zero pixeld search until the last sliding windows.\n",
    "\n",
    "```python\n",
    "def get_histogram(img):\n",
    "    # Grab only the bottom half of the image\n",
    "    # Lane lines are likely to be mostly vertical nearest to the car\n",
    "    bottom_half = img[img.shape[0]//2:,:] # floor division\n",
    "    \n",
    "    # Sum across image pixels vertically - make sure to set `axis`\n",
    "    # i.e. the highest areas of vertical lines should be larger values\n",
    "    # along all the columns in the lower half of the image\n",
    "    # The image data. The returned array has shape (M, N) for grayscale images.\n",
    "    histogram = np.sum(bottom_half, axis=0)\n",
    "    \n",
    "    return histogram\n",
    "```\n",
    "\n",
    "<table><tr><td><img src='output_images/histogram_straight_lines1.png'></td><td><img src='output_images/histogram_straight_lines2.png'></td></tr></table>\n",
    "\n",
    "<table><tr><td><img src='output_images/histogram_test2.png'></td><td><img src='output_images/histogram_test3.png'></td></tr></table>\n",
    "\n",
    "<table><tr><td><img src='output_images/lane_pixels_detection_straight_lines1.png'></td><td><img src='output_images/lane_pixels_detection_straight_lines2.png'></td></tr></table>\n",
    "<table><tr><td><img src='output_images/lane_pixels_detection_test1.png'></td><td><img src='output_images/lane_pixels_detection_test2.png'></td></tr></table>\n",
    "\n",
    "\n",
    "```python\n",
    "def detect_lane_lines(img, output_img=False):\n",
    "    \n",
    "    show_search_window = True\n",
    "        \n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = get_histogram(img)\n",
    "\n",
    "    if output_img:\n",
    "        # Create an output image to draw on and  visualize the result\n",
    "        out_img = np.dstack((img, img, img))*255\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(img.shape[0]//nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero  = img.nonzero()\n",
    "    nonzerox = np.array(nonzero[1])   \n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "   \n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    \n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = img.shape[0] - (window + 1)*window_height\n",
    "        win_y_high = img.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        \n",
    "        if output_img:\n",
    "            #Draw the windows on the visualization image\n",
    "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high), (0,255,0), 3) \n",
    "            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high), (0,255,0), 3) \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                          (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                           (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    \n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "    \n",
    "    ploty, left_fitx, right_fitx = fit_polynomial_pixel_space(img.shape,leftx,lefty,rightx,righty)\n",
    "    \n",
    "    if output_img:\n",
    "        # Draw left and right pixels in colors\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "        \n",
    "        # Draw the line fit for the left and right lane\n",
    "        for index in range(img.shape[0]):\n",
    "            cv2.circle(out_img, (int(left_fitx[index]), int(ploty[index])), 3, (255,255,0))\n",
    "            cv2.circle(out_img, (int(right_fitx[index]), int(ploty[index])), 3, (255,255,0))\n",
    "\n",
    "        if show_search_window:\n",
    "            # Generate a polygon to illustrate the search window area\n",
    "            # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "            left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "            left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, \n",
    "                                      ploty])))])\n",
    "            left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "            right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "            right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, \n",
    "                                      ploty])))])\n",
    "            right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "            # Draw the lane onto the warped blank image\n",
    "            window_img = np.zeros_like(out_img)\n",
    "            cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "            cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "            out_img = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "\n",
    "\n",
    "        return ploty, left_fitx, right_fitx, out_img.astype(int)\n",
    "    return ploty, left_fitx, right_fitx\n",
    "```\n",
    "\n",
    "#### 4.2 Fitting the lane lines using 2nd order polynomial\n",
    "\n",
    "Once the lane line pixels were identified in the previous step, I used the `fit_polynomial_pixel_space(img_shape,left_fit,right_fit)` to find the line fit for each lane line. This 2nd order polynomial line-fit takes image size, the lists of lane line pixels as input argument and predicts the x position of the left and right lane line.\n",
    "\n",
    "```python\n",
    "def fit_polynomial_pixel_space(img_shape,leftx,lefty,rightx,righty):\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img_shape[0] - 1, img_shape[0])\n",
    "    \n",
    "    try:\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "    \n",
    "    return ploty, left_fitx, right_fitx\n",
    "\n",
    "ploty, left_fitx, right_fitx = fit_polynomial_pixel_space(img.shape,left_fit,right_fit)\n",
    "leftx = left_fitx\n",
    "rightx = right_fitx\n",
    "\n",
    "if output_img:\n",
    "    # Draw left and right pixels in colors\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "    #Draw the windows on the visualization image\n",
    "    #cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high), (0,255,0), 3) \n",
    "    #cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high), (0,255,0), 3) \n",
    "\n",
    "    # Generate a polygon to illustrate the search window area\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, \n",
    "                              ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, \n",
    "                              ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    window_img = np.zeros_like(out_img)\n",
    "    cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "    out_img = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "\n",
    "    #Plot the polynomial lines onto the image\n",
    "    plt.plot(left_fitx, ploty, color='yellow')\n",
    "    plt.plot(right_fitx, ploty, color='yellow')\n",
    "\n",
    "    return ploty, leftx, rightx, out_img.astype(int)\n",
    "```\n",
    "\n",
    "As a result of the lane detection, the predicted lane lines are drawn in yellow, see below.\n",
    "\n",
    "<table><tr><td><img src='output_images/polyfit_straight_lines1.png'></td><td><img src='output_images/polyfit_straight_lines2.png'></td></tr></table>\n",
    "<table><tr><td><img src='output_images/polyfit_test1.png'></td><td><img src='output_images/polyfit_test2.png'></td></tr></table>\n",
    "\n",
    "#### 5. Determine the curvature of the lane and vehicle position with respect to center.\n",
    "\n",
    "Before we can determine the lane curvature in meters, first we need to convert the x and y coordinates from pixels space to meters. This involves measuring how long and wide the section of lane is that we're projecting in our warped image. We could do this in detail by measuring out the physical lane in the field of view of the camera, I assumed that the lane is about 30 meters long and 3.7 meters wide. Alternatively I could have derived the conversion from pixel space to world space in my images, compare the images with U.S. regulations that require a minimum lane width of 12 feet or 3.7 meters, and the dashed lane lines are 10 feet or 3 meters long each.\n",
    "\n",
    "My source camera image has 720 pixels in the y-dimension, bear in mind that the image was perspective-transformed, so I took some pixels off, say 700 pixels in the y-dimension. Therefore, to convert from pixels to real-world meter measurements, I used:\n",
    "\n",
    "```python\n",
    "# Define conversions in x and y from pixels space to meters\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "```\n",
    "\n",
    "I defined a function `fit_polynomial_world_space(img_shape, leftx, rightx, ym_per_pix, xm_per_pix)` that takes the range of y pixels, the left and right lanes non-zero pixels and scaling of pixels to meters, and output the polynomial coefficients for lane and right lanes that predicts the x position in meters.\n",
    "\n",
    "```python\n",
    "def fit_polynomial_world_space(ploty, leftx, rightx, ym_per_pix, xm_per_pix):\n",
    "    leftx = leftx[::-1]    # Reverse to match top-to-bottom in y\n",
    "    rightx = rightx[::-1]  # Reverse to match top-to-bottom in y\n",
    "    \n",
    "    try:        \n",
    "        # Fit polynomials to x,y in world space\n",
    "        left_fit_cr  = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "        \n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "    \n",
    "    # Second-order polynomial to predict y position\n",
    "    # f(y) = Ay^2 + By + Cf(y)\n",
    "    left_A_coeff = left_fit_cr[0]\n",
    "    left_B_coeff = left_fit_cr[1]\n",
    "    left_C_coeff = left_fit_cr[2] # not used for prediction\n",
    "    \n",
    "    right_A_coeff = right_fit_cr[0]\n",
    "    right_B_coeff = right_fit_cr[1]\n",
    "    right_C_coeff = right_fit_cr[2] # not used for prediction\n",
    "    \n",
    "    return left_A_coeff, left_B_coeff, right_A_coeff, right_B_coeff\n",
    "```\n",
    "\n",
    "To calculate the radius of lane curvature, simply use the radius of curvature formula, which implemented in the `compute_curvature(img_shape, leftx, rightx, ym_per_pix, xm_per_pix)`\n",
    "\n",
    "![alt_text](R_curve.png)\n",
    "\n",
    "```python\n",
    "def compute_curvature(img_shape, leftx, rightx, ym_per_pix, xm_per_pix):\n",
    "    ploty = np.linspace(0, img_shape[0]-1, img_shape[0]) # to cover same y-range as image\n",
    "    \n",
    "    # Polynomial line fit in meters (not pixel space any more)\n",
    "    left_A_coeff, left_B_coeff, right_A_coeff, right_B_coeff = fit_polynomial_world_space(ploty, leftx, rightx, ym_per_pix, xm_per_pix)\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # Choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)*ym_per_pix\n",
    "    \n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    left_curverad = ((1 + (2*left_A_coeff*y_eval + left_B_coeff)**2)**1.5) / np.absolute(2*left_A_coeff)\n",
    "    right_curverad = ((1 + (2*right_A_coeff*y_eval + right_B_coeff)**2)**1.5) / np.absolute(2*right_A_coeff)\n",
    "    \n",
    "    # Estimated lane curvature\n",
    "    radius_of_curvature = (left_curverad + right_curverad)/2\n",
    "    \n",
    "    # The radius of curvature in meters for both lane lines\n",
    "    return radius_of_curvature, left_curverad, right_curverad\n",
    "```\n",
    "\n",
    "Vehicle offset\n",
    "\n",
    "Assuming the camera is mounted at the center of the car, such that the lane center is the midpoint at the bottom of the image between the two lines you've detected. The offset of the lane center from the center of the image (converted from pixels to meters) is your distance from the center of the lane.\n",
    "\n",
    "To cacluate the vehicle offset, calculate the x position between the left and right lane and measure the postion relative to the mid point of the image, then scale it with the x-pixel to meters conversion.\n",
    "\n",
    "```python\n",
    "def compute_veh_offset(img_shape, leftx, rightx, xm_per_pix):\n",
    "    midpointx = img_shape[1]//2\n",
    "        \n",
    "    # Vehicle position with respect to the lane\n",
    "    # Lane center is the midpoint at the bottom of the image between the two lines detected.\n",
    "    veh_posx = (leftx[-1] + rightx[-1])/2\n",
    "    \n",
    "    # Horizontal offset \n",
    "    veh_offsetx = (midpointx - veh_posx) * xm_per_pix\n",
    "\n",
    "    return veh_offsetx\n",
    "```\n",
    "\n",
    "###  6. Warp the Detected Lane Boundary and display of the lane boundaries\n",
    "Warp the detected lane boundaries back onto plotted back down onto the road image. I implemented this step using `draw_line()`. Here is an example of my result on a test image:\n",
    "\n",
    "```python\n",
    "def draw_lane(img,combined_warped,Minv,ploty,left_fitx,right_fitx):\n",
    "           \n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(combined_warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    new_warp = cv2.warpPerspective(color_warp, Minv, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Combine the result with the original image\n",
    "    out_img = cv2.addWeighted(img, 1, new_warp, 0.3, 0)\n",
    "\n",
    "    return out_img\n",
    "```\n",
    "\n",
    "<table><tr><td><img src='output_images/detected_boundary_straight_lines1.jpg'></td><td><img src='output_images/detected_boundary_straight_lines2.jpg'></td></tr></table>\n",
    "<table><tr><td><img src='output_images/detected_boundary_test1.jpg'></td><td><img src='output_images/detected_boundary_test2.jpg'></td></tr></table>\n",
    "<table><tr><td><img src='output_images/detected_boundary_test3.jpg'></td><td><img src='output_images/detected_boundary_test4.jpg'></td></tr></table>\n",
    "<table><tr><td><img src='output_images/detected_boundary_test5.jpg'></td><td><img src='output_images/detected_boundary_test6.jpg'></td></tr></table>\n",
    "\n",
    "\n",
    "### 7. Display estimation of lane curvature and vehicle position\n",
    "\n",
    "At the last step of the pipeline, I overlayed the results obtained from the `compute_curvature()` and `compute_veh_offset` to the image.\n",
    "\n",
    "```python\n",
    "def display_estimation(img, radius_of_curvature, veh_offsetx):\n",
    "\n",
    "    # Display lane curvature\n",
    "    out_img = img.copy()\n",
    "    cv2.putText(out_img, 'Radius of Curvature: {:.0f} (m)'.format(radius_of_curvature), \n",
    "                (60, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 5)\n",
    "    \n",
    "    # Display vehicle offset\n",
    "    if veh_offsetx < 0.0: \n",
    "        cv2.putText(out_img, 'Vehicle is {:.2f}m left of center'.format(np.absolute(veh_offsetx)), \n",
    "                    (60, 110), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 5)\n",
    "    else:\n",
    "        cv2.putText(out_img, 'Vehicle is {:.2f}m right of center'.format(veh_offsetx), \n",
    "                    (60, 110), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 5)\n",
    "    \n",
    "    return out_img\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline (Image)\n",
    "\n",
    "```python\n",
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line:\n",
    "    # Initalize the class\n",
    "    def __init__(self, images,always_on_calibrate=True):\n",
    "        # Define conversions in x and y from pixels space to meters\n",
    "        self.ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "        self.xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "        if always_on_calibrate:\n",
    "            # Calibrate camera\n",
    "            self.ret, self.mtx, self.dist, self.rvecs, self.tvecs = compute_camera_calibration(images)\n",
    "            \n",
    "            # Optional save the camera calibration as pickle file\n",
    "            save_camera_calibration(self.ret, self.mtx, self.dist, self.rvecs, self.tvecs)\n",
    "        else:\n",
    "            # Load previous camera calibration\n",
    "            self.ret, self.mtx, self.dist, self.rvecs, self.tvecs = \\\n",
    "            load_camera_calibration(camera_calibration='camera_cal/camera_cal.p')\n",
    "        \n",
    "        # Binary image thresholds\n",
    "        self.orient = 'x'\n",
    "        self.sobel_kernel = 3\n",
    "        self.sx_thresh = [20, 100]\n",
    "        self.s_thresh = [170, 255]\n",
    "\n",
    "        # Pick 4 points to transform\n",
    "        # Note: src and dst points are in [x,y] format\n",
    "        self.src = np.float32(\n",
    "        [[245,  680],  # Bottom left\n",
    "         [580,  460],  # Top left\n",
    "         [710,  460],  # Top right\n",
    "         [1060, 680]]) # Bottom right\n",
    "\n",
    "        self.dst = np.float32(\n",
    "        [[245,  680],  # Bottom left\n",
    "         [245,    0],  # Top left\n",
    "         [1060,   0],  # Top right\n",
    "         [1060, 680]]) # Bottom right\n",
    "    \n",
    "        # The perspective transform matrices\n",
    "        self.M = cv2.getPerspectiveTransform(self.src, self.dst)\n",
    "        self.Minv = cv2.getPerspectiveTransform(self.dst, self.src)\n",
    "        \n",
    "        self.lines_fit = None\n",
    "        \n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = []\n",
    "        \n",
    "        #x values for detected line pixels\n",
    "        #self.allx = None  \n",
    "        \n",
    "        #y values for detected line pixels\n",
    "        #self.ally = None\n",
    "        \n",
    "        self.ploty = None\n",
    "        self.leftx = None\n",
    "        self.rightx = None\n",
    "        \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None   \n",
    "        \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None\n",
    "        \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        \n",
    "    # This is a special function active during the life time of the execution    \n",
    "    def __call__(self, img):\n",
    "        \n",
    "        # Apply the image processing to the source image\n",
    "        out_img = self.pipeline(img)\n",
    "        return out_img\n",
    "    \n",
    "    def pipeline(self,img):\n",
    "        # 1. Calibrate camera (see init method)\n",
    "        \n",
    "        # 2. Undistort the image\n",
    "        undist_img = undistort_camera_img(img,mtx=self.mtx,dist=self.dist)\n",
    "        out_img = undist_img\n",
    "        #plot_images(img,undist_img,title1='Camera Image',title2='Undistorted Image')\n",
    "        \n",
    "        # 3. Threholding binary images\n",
    "        # Create gradient-directional threshold binary image\n",
    "        sx_binary = abs_sobel_thresh(undist_img,self.orient,self.sobel_kernel,self.sx_thresh)\n",
    "        #plot_images(undist_img,sx_binary,title1='Undisorted Camera Image',title2='Gradient-directional binary')\n",
    "        \n",
    "        # Create saturation threshold binary image\n",
    "        s_binary = saturation_thresh(undist_img,self.s_thresh)\n",
    "        #plot_images(undist_img,s_binary,title1='Undisorted Camera Image',title2='Saturation binary')\n",
    "        \n",
    "        # Stacked binary\n",
    "        color_binary = np.dstack((np.zeros_like(sx_binary), sx_binary, s_binary)) * 255\n",
    "        # Combined binary\n",
    "        combined_binary = combine_binary_thresh(sx_binary,s_binary)\n",
    "        #plot_images(color_binary,combined_binary,title1='Stacked binary',title2='Combined binary')\n",
    "        \n",
    "        # 4. Perspective Transform\n",
    "        warped_img = warp(undist_img,self.M)\n",
    "        #plot_images(undist_img,warped_img,title1=None,title2=None)\n",
    "        combined_warped = warp(combined_binary,self.M)\n",
    "        #plot_images(warped_img,combined_warped,title1='Bird-eye view',title2='Bird-eye view binary')\n",
    "        \n",
    "        # 5. Detect lane lines\n",
    "        self.ploty, self.leftx, self.rightx = detect_lane_lines(combined_warped)\n",
    "        # Disable the output image for the detect_lane_lines to save computation\n",
    "        #self.ploty, self.leftx, self.rightx, out_img = detect_lane_lines(combined_warped, output_img=True)\n",
    "        \n",
    "        # 6a. Calculate lane curvature\n",
    "        self.radius_of_curvature, self.left_curvature, self.right_curvature = \\\n",
    "            compute_curvature(combined_warped.shape, self.leftx, self.rightx,\\\n",
    "                                          self.ym_per_pix, self.xm_per_pix)\n",
    "\n",
    "        # 6b. Calculate the vehicle offset relative to the lane center\n",
    "        self.line_base_pos = compute_veh_offset(img.shape, self.leftx, self.rightx, self.xm_per_pix)\n",
    "        \n",
    "        # 7. Draw lane lines on the image\n",
    "        lane_img = draw_lane(img,combined_warped,self.Minv,self.ploty,self.leftx,self.rightx)\n",
    "        out_img = lane_img\n",
    "              \n",
    "        # 8. Display the lane curvature and vehicle offset (to the lane)\n",
    "        out_img = display_estimation(lane_img, self.radius_of_curvature, self.line_base_pos)\n",
    "        \n",
    "        # Free text overlay the image\n",
    "        out_img = display(out_img,'')\n",
    "        \n",
    "        return out_img\n",
    "```\n",
    "\n",
    "<table><tr><td><img src='output_images/annotated_test1.jpg'></td><td><img src='output_images/annotated_test2.jpg'></td></tr></table>\n",
    "<table><tr><td><img src='output_images/annotated_test3.jpg'></td><td><img src='output_images/annotated_test4.jpg'></td></tr></table>\n",
    "<table><tr><td><img src='output_images/annotated_test5.jpg'></td><td><img src='output_images/annotated_test6.jpg'></td></tr></table>\n",
    "\n",
    "### Pipeline (video)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"output_videos/project_video.gif\">\n",
    "  <a href=\"output_videos/project_video.mp4\">Here's a link to my video result</a>\n",
    "</p>\n",
    "\n",
    "```python\n",
    "input_video = 'test_videos/project_video.mp4'\n",
    "output_video = 'output_videos/output_project_video.mp4'\n",
    "\n",
    "preprocess_clip = VideoFileClip(input_video)\n",
    "# preprocess_clip = VideoFileClip(input_video).subclip(0,5.0) #or subclip with the first 5 seconds\n",
    "\n",
    "# Create a new 'Line' object\n",
    "camera_cal_imgs = 'camera_cal/calibration*.jpg'\n",
    "process_image = Line(camera_cal_imgs)\n",
    "\n",
    "# Process video frames with the 'process_image' function\n",
    "postprocess_clip = preprocess_clip.fl_image(process_image)\n",
    "\n",
    "# Write the output video to the output_video file\n",
    "%time postprocess_clip.write_videofile(output_video, audio=False)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion on pipeline\n",
    "\n",
    "#### Problems / issues  in the current implementation of this project, potential solution and future work\n",
    "\n",
    "Currently the lane detection pipeline is implemented with the minimum algorithm possible for basic lane detection like one shown with the `project_video`. In summary of the pipeline, the camera image is first undistorted then it goes through a series of threshold filters to extract useful lane lines features to a binary image. This extracted binary image is then transformed to a bird-eye view. After that a simple algorithm is used to find the location of high density yellow and white pixels (recognized as lane lines) in a number of sliding window. The sliding window is used to remember the last lane position if the minimum number of pixels are not found in the next sliding window (i.e. bad frame or broken lane lines).\n",
    "\n",
    "After the raw lane lines are detected, I then used a 2nd-order polynomial to fit in the line curve. In other words to predict the lane curve for the next video frame, this is useful particularly if there are bad frames or lines are not seem over a certain time horizon timeframe.\n",
    "\n",
    "After the pixel space polynomial is obtained, I re-scaled the polynomial to work in the world space (in meters not pixel). Using this new polynomial, I estimatd the radius of curvature and the vehicle offset to the lane.\n",
    "\n",
    "At last, I drew the detected lane lines using the pixel space polynomial and project it back to the lane line using inverse perspective transform, so I used the perspective tranform with the \"inverse\" M matrix instead. Also overlay the estimated road curvature.\n",
    "\n",
    "While the current lane detection pipeline works in certain conditions, more work needs to be done to improve the pipeline to make it work more robustly in more operating conditions. There are couple of weakest links in the current pipeline implementation. For example, it is not effective in reseting when it encounters no line or bad frames, rejection to shadows, other lane markings and nearby vehicles.\n",
    "\n",
    "#### Not real-time ready yet\n",
    "The pipeline takes about 2 minutes to process a 50 sec video clip, which could pose issue if we are taking this processing pipeline to use it in real-time. One way to reduce the computation is to remember the last search sliding window, this will greatly reduce the needs to blindly search the sliding windows for each sliding window.\n",
    "\n",
    "#### Reset Mechanism\n",
    "Currently my pipeline is a simple implementation, it does not have reset mechanism if there are bad or difficult frame of video, I can make it more robust by retain the previous positions from the frame prior and step to the next frame to search again. If I lose the lines for several frames in a row, I should probably start searching from scratch using a histogram and sliding window, or another method, to re-establish your measurement.\n",
    "\n",
    "#### Smoothing\n",
    "Despite my pipeline currently works well in the simple lighting conditions with good lane lines marking, currently my line detections will jump around from frame to frame a bit. To make the lane detection pipeline smoother, I can**smooth over the last n frames of video** to obtain a cleaner result. Each time I get a new high-confidence measurement, I can append it to the list of recent measurements and then **take an average over n past measurements** to obtain the lane position you want to draw onto the image.\n",
    "\n",
    "#### Selection of hyper parameters requires lots of tuning (i.e. minimum pixels to recognize lane lines, the number of vertical sliding windows, windows margin, etc).\n",
    "The selection of the hyper parameters to tune to detect the lane lines can be tricky to make it just \"right\" to give just sensitive enough to detect any lane lines but strong enough to reject the road noises. For example, if a minimum pixels to recognize is too small, the detection of the lane lines will be very poor hence greatly affects the reliability of the road curvature radius.\n",
    "\n",
    "### Discussion on challenges to make the lane detection work robustly\n",
    "\n",
    "In the `challenge_video`, the current algorithm will give unreliable road curvature and the vehicle offset most of the time when there is ...\n",
    "\n",
    "| External Enivronment                     | Potential solution for better robustness   | \n",
    "|:----------------------------------------:|:------------------------------------------:| \n",
    "| Shadow of the curb or bridge             | Tune threshold binary to reject non yellow and white pixels | \n",
    "| Dual colors in pavement in the lane      | Tune threshold binary to reject non yellow and white pixels |\n",
    "| (Dark gray) oil drip on the road         | Tune threshold binary to reject non yellow and white pixels |\n",
    "| Other road markings in the lane          | Tune threshold binary to reject horzontal lines             |\n",
    "| Horziontal lines on the road             | Tune threshold binary to reject horzontal lines             |\n",
    "| A nearby car gets very close to the lane | Tune min pixels to recognize lane lines, skip window search for bad frame|\n",
    "\n",
    "\n",
    "In the `harder_challenge_video`, the current algorithm will give unreliable road curvature and the vehicle offset  most of the time when there is ...\n",
    "\n",
    "\n",
    "| External Enivronment                     | Potential solution for better robustness   | \n",
    "|:----------------------------------------:|:------------------------------------------:| \n",
    "| Motorist changing into your lane         | Reset and smoothing over the last n windows                 |\n",
    "| Reflection of the dashboard to wind sceen| Tune threshold binary to reject close to horizontal lines   |\n",
    "| Bright road curbs                        | Tune threshold binary to reject non yellow and white pixels | \n",
    "| Patches of tree shadows on the road      | Tune threshold binary to reject non yellow and white pixels | \n",
    "| Windsheild/camera cleaningness           | Tune threshold binary to reject non yellow and white pixels |\n",
    "| Sun glare                                | Tune threshold binary to reject non yellow and white pixels | \n",
    "| Sharp turn                               | Reset and smoothing over the last n windows                 | \n",
    "| Oncoming traffic with/without headlight  | TBD                                                         |\n",
    "| Double lane lines                        | TBD                                                         |\n",
    "\n",
    "Other potential problems that are not found in the challenge videos.\n",
    "\n",
    "| External Enivronment                     | Potential solution for better robustness   | \n",
    "|:----------------------------------------:|:------------------------------------------:| \n",
    "| Wet roads (reflection on the road)       | TBD | \n",
    "| Close to a car in front                  | TBD |\n",
    "| White or yellow motorist in the lane     | TBD | \n",
    "| Night                                    | TBD | \n",
    "| Snow                                     | TBD | \n",
    "| Rainy                                    | TBD | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
